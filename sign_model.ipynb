{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "intimate-brisbane",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "phantom-hawaii",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'data/signs/train'\n",
    "test_path = 'data/signs/test'\n",
    "valid_path = 'data/signs/valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "respected-swiss",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9993 images belonging to 36 classes.\n",
      "Found 1080 images belonging to 36 classes.\n",
      "Found 180 images belonging to 36 classes.\n"
     ]
    }
   ],
   "source": [
    "train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input).flow_from_directory(\n",
    "    directory=train_path, target_size=(224,224), batch_size=10)\n",
    "valid_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input).flow_from_directory(\n",
    "    directory=valid_path, target_size=(224,224), batch_size=10)\n",
    "test_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input).flow_from_directory(\n",
    "    directory=test_path, target_size=(224,224), batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "prerequisite-newcastle",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Sequential([\n",
    "    Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same', input_shape=(224, 224, 3)),\n",
    "    MaxPool2D(pool_size=(2,2), strides=2),\n",
    "    Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same'),\n",
    "    MaxPool2D(pool_size=(2,2), strides=2),\n",
    "    Flatten(),\n",
    "    Dense(units=36, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "sacred-album",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fabulous-wrist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 224, 224, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 112, 112, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 200704)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 36)                7225380   \n",
      "=================================================================\n",
      "Total params: 7,244,772\n",
      "Trainable params: 7,244,772\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "confused-person",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 - 357s - loss: 2.4590 - accuracy: 0.3399 - val_loss: 1.8720 - val_accuracy: 0.4852\n",
      "Epoch 2/10\n",
      "1000/1000 - 539s - loss: 0.5810 - accuracy: 0.8455 - val_loss: 2.0637 - val_accuracy: 0.4750\n",
      "Epoch 3/10\n",
      "1000/1000 - 587s - loss: 0.0326 - accuracy: 0.9940 - val_loss: 2.9495 - val_accuracy: 0.4704\n",
      "Epoch 4/10\n",
      "1000/1000 - 342s - loss: 0.0049 - accuracy: 0.9989 - val_loss: 3.1277 - val_accuracy: 0.4824\n",
      "Epoch 5/10\n",
      "1000/1000 - 347s - loss: 0.0110 - accuracy: 0.9972 - val_loss: 3.7976 - val_accuracy: 0.4093\n",
      "Epoch 6/10\n",
      "1000/1000 - 363s - loss: 0.0056 - accuracy: 0.9986 - val_loss: 5.1222 - val_accuracy: 0.3639\n",
      "Epoch 7/10\n",
      "1000/1000 - 336s - loss: 0.0232 - accuracy: 0.9933 - val_loss: 4.4366 - val_accuracy: 0.4222\n",
      "Epoch 8/10\n",
      "1000/1000 - 344s - loss: 0.0071 - accuracy: 0.9979 - val_loss: 4.8200 - val_accuracy: 0.4278\n",
      "Epoch 9/10\n",
      "1000/1000 - 362s - loss: 0.0147 - accuracy: 0.9953 - val_loss: 5.2495 - val_accuracy: 0.3870\n",
      "Epoch 10/10\n",
      "1000/1000 - 385s - loss: 0.0075 - accuracy: 0.9978 - val_loss: 5.6891 - val_accuracy: 0.4093\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x16e366c8240>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(x=train_batches, validation_data=valid_batches, epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "arctic-lover",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  2,  2,  2,  2,  2,  3,  3,\n",
       "        3,  3,  3,  4,  4,  4,  4,  4,  5,  5,  5,  5,  5,  6,  6,  6,  6,\n",
       "        6,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9, 10,\n",
       "       10, 10, 10, 10, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 13, 13, 13,\n",
       "       13, 13, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16,\n",
       "       17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 20, 20,\n",
       "       20, 20, 20, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 23, 23, 23, 23,\n",
       "       23, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 27,\n",
       "       27, 27, 27, 27, 28, 28, 28, 28, 28, 29, 29, 29, 29, 29, 30, 30, 30,\n",
       "       30, 30, 31, 31, 31, 31, 31, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33,\n",
       "       34, 34, 34, 34, 34, 35, 35, 35, 35, 35])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batches.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "otherwise-groove",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model1.predict(x=test_batches, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "attended-dietary",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true=test_batches.classes, y_pred=np.argmax(predictions, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "sensitive-malawi",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                        normalize=False,\n",
    "                        title='Confusion matrix',\n",
    "                        cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "essential-welcome",
   "metadata": {},
   "outputs": [],
   "source": [
    "category = list(test_batches.class_indices.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "small-savannah",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n"
     ]
    }
   ],
   "source": [
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true=test_batches.classes, y_pred=np.argmax(predictions, axis=-1))\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm, labels=category, title='Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-queens",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
